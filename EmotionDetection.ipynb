{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PradNir/Gen-AI-Driven-Cover-Letter-and-Cold-Email-Generator-for-Job-Applications/blob/main/EmotionDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-fH7c7SL81u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import networkx as nx\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJMI2R6LPBzK"
      },
      "outputs": [],
      "source": [
        "# Load text data\n",
        "text_data = pd.read_csv('/content/drive/MyDrive/EmotionDetection/iemocap_full_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess image\n",
        "def preprocess_image(image):\n",
        "    # Resize image to a fixed size (e.g., 48x48 pixels)\n",
        "    resized_image = cv2.resize(image, (48, 48))\n",
        "    # Convert image to grayscale\n",
        "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    normalized_image = grayscale_image / 255.0\n",
        "    return normalized_image"
      ],
      "metadata": {
        "id": "vQZ-PLeBxDEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract label from filename\n",
        "def extract_label(filename):\n",
        "    # Extract emotion label from filename\n",
        "    label = int(filename.split('.')[0][-2:])\n",
        "    return label\n"
      ],
      "metadata": {
        "id": "OI0efZiXxj4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load labels\n",
        "labels = []\n",
        "for filename in os.listdir('/content/drive/MyDrive/EmotionDetection/CK+'):\n",
        "    if filename.endswith('.png'):\n",
        "        label = extract_label(filename)\n",
        "        labels.append(label)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "CgjJan59xqAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the directory containing CK+ images\n",
        "ck_plus_images_dir = '/content/drive/MyDrive/EmotionDetection/CK+'"
      ],
      "metadata": {
        "id": "FN_cXg-Ay6Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Define a mapping of emotion labels to numerical values\n",
        "emotion_label_map = {\n",
        "    'Anger': 0,\n",
        "    'Contempt': 1,\n",
        "    'Disgust': 2,\n",
        "    'Fear': 3,\n",
        "    'Happy': 4,\n",
        "    'Sadness': 5,\n",
        "    'Surprise': 6\n",
        "}\n",
        "\n",
        "\n",
        "# Loop through each directory (corresponding to each emotion label)\n",
        "for emotion_dir in os.listdir(ck_plus_images_dir):\n",
        "    # Skip non-directory files\n",
        "    if not os.path.isdir(os.path.join(ck_plus_images_dir, emotion_dir)):\n",
        "        continue\n",
        "\n",
        "    # Map the emotion label to its numerical value\n",
        "    label = emotion_label_map.get(emotion_dir)\n",
        "    if label is None:\n",
        "        continue  # Skip if the emotion label is not defined in the mapping\n",
        "\n",
        "    # Loop through each image file in the directory\n",
        "    for filename in os.listdir(os.path.join(ck_plus_images_dir, emotion_dir)):\n",
        "        if filename.endswith('.png'):  # Assuming images are in PNG format\n",
        "            # Read the image\n",
        "            image = cv2.imread(os.path.join(ck_plus_images_dir, emotion_dir, filename))\n",
        "            if image is None:\n",
        "                continue  # Skip if image loading fails\n",
        "            # Preprocess the image\n",
        "            preprocessed_image = preprocess_image(image)\n",
        "            # Add the preprocessed image to the list of images\n",
        "            images.append(preprocessed_image)\n",
        "            # Add the label to the list of labels\n",
        "            labels.append(label)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "TxGvjvGB04tc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "db160faa-638d-4cdd-8728-ef1821536a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f9d6977fccec>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Split the dataset into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Support Vector Machine (SVM) classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train.reshape(len(X_train), -1), y_train)"
      ],
      "metadata": {
        "id": "4efW31Ij3Yon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KJ8EN7W94MSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train.reshape(len(X_train), -1), y_train)"
      ],
      "metadata": {
        "id": "J6jV_gUR7fef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "clusters = kmeans.fit_predict(X_train.reshape(len(X_train), -1))"
      ],
      "metadata": {
        "id": "0Oz_F1Fz6PxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model for facial expression analysis\n",
        "def build_cnn_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(7, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ekylw9uz7t40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "cnn_model = build_cnn_model()\n",
        "cnn_model.fit(X_train.reshape(-1, 48, 48, 1), y_train, epochs=10)"
      ],
      "metadata": {
        "id": "8zCZWmwV6qFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate SVM model\n",
        "svm_accuracy = svm_classifier.score(X_test.reshape(len(X_test), -1), y_test)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "rf_accuracy = rf_classifier.score(X_test.reshape(len(X_test), -1), y_test)\n",
        "\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)\n"
      ],
      "metadata": {
        "id": "p75z7i0g9Oqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate CNN model\n",
        "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test.reshape(-1, 48, 48, 1), y_test)\n",
        "\n",
        "print(\"CNN Accuracy:\", cnn_accuracy)\n"
      ],
      "metadata": {
        "id": "gnXXxnx69R0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the pre-trained CNN model\n",
        "cnn_model.save('emotion_recognition_model.h5')\n"
      ],
      "metadata": {
        "id": "-uTZnTvN-5J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Load the pretrained CNN model for emotion detection\n",
        "cnn_model = tf.keras.models.load_model('/content/emotion_recognition_model.h5')\n",
        "\n",
        "# Define the emotion labels\n",
        "emotion_labels = {0: 'Anger', 1: 'Contempt', 2: 'Disgust', 3: 'Fear', 4: 'Happy', 5: 'Sadness', 6: 'Surprise'}\n",
        "\n",
        "# Function to detect emotion from an image frame\n",
        "def detect_emotion(frame):\n",
        "    # Preprocess the frame\n",
        "    resized_frame = cv2.resize(frame, (48, 48))\n",
        "    grayscale_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
        "    normalized_frame = grayscale_frame / 255.0\n",
        "    input_frame = np.expand_dims(normalized_frame, axis=-1)\n",
        "    input_frame = np.expand_dims(input_frame, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Perform emotion prediction\n",
        "    predictions = cnn_model.predict(input_frame)\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    emotion_label = emotion_labels[predicted_class]\n",
        "\n",
        "    return emotion_label\n",
        "\n",
        "# Function to perform real-time emotion detection from the camera\n",
        "def real_time_emotion_detection():\n",
        "    # Initialize the camera\n",
        "    camera = cv2.VideoCapture(0)\n",
        "\n",
        "    while True:\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = camera.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Detect emotion in the frame\n",
        "        emotion_label = detect_emotion(frame)\n",
        "\n",
        "        # Display the emotion label on the frame\n",
        "        cv2.putText(frame, emotion_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Display the frame\n",
        "        cv2.imshow('Real-time Emotion Detection', frame)\n",
        "\n",
        "        # Break the loop when 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release the camera and close all OpenCV windows\n",
        "    camera.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Perform real-time emotion detection\n",
        "real_time_emotion_detection()\n"
      ],
      "metadata": {
        "id": "wglCyDd6VIbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnn_model.summary())\n"
      ],
      "metadata": {
        "id": "o7GIqacHViD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "isstgb3kYEpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize camera\n",
        "camera = cv2.VideoCapture(0)"
      ],
      "metadata": {
        "id": "UAxqKipUf0SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = camera.read()\n",
        "\n",
        "    # Check if frame capture was successful\n",
        "    if not ret:\n",
        "        print(\"Error: Failed to capture frame from the camera.\")\n",
        "        break\n",
        "# Display the frame\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # Break the loop when 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera and close OpenCV windows\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "ts0kUw2Hf5Kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1X2dsguHKAfBqzXblqejXZdrqIdJm-i56",
      "authorship_tag": "ABX9TyMBZmpaUFsTgPZCuB1ytMZJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}